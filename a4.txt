# ANSWERS & RESULTS

# 5) Use the parallel command we have seen to perform this operation against all 100 REVIEWS/reviewID.txt files in parallel. 
#    Time your parallel execution with the time command. How long did it take?

	# First I tested on individual files and subsets of data to get a sense of how long it might take: 
	
	benbow@f6linuxA5:~/a4$ head -n 100 just-tweet-bodies.copy > just-tweet-bodies.txt
	benbow@f6linuxA5:~/a4$ time bash append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt
	
	real    0m0.430s
	user    0m0.445s
	sys     0m0.213s
	
	benbow@f6linuxA5:~/a4$ head -n 1000 just-tweet-bodies.copy > just-tweet-bodies.txt
	benbow@f6linuxA5:~/a4$ time bash append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt
	
	real    0m6.345s
	user    0m4.764s
	sys     0m4.082s

	benbow@f6linuxA5:~/a4$ head -n 10000 just-tweet-bodies.copy > just-tweet-bodies.txt
	benbow@f6linuxA5:~/a4$ time bash append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt
	
	real    1m18.411s
	user    0m48.230s
	sys     1m1.651s

	# Run time increased by a factor of 14.76 from n=100 to n=1000, then increased by a factor of 12.36 from n=1000 to n=10000
	
	# I struggled to install parallel, so I tried it on another server where I could install parallel and ran it with 500 tweets:
	
	head -n 500 just-tweet-bodies.copy > just-tweet-bodies.txt
	time parallel bash append-if-2-common-words.sh {1} just-tweet-bodies.txt :::: unique-review-IDs.txt
	
	# It took 40 seconds with this small subset of 500 tweets. 
	# I think it would scale somewhat linearly with all 1,600,000 tweets in the full data set, 
	# which would result in a runtime of about 35.6 hours.

# 6) Amongst all the tweets you filtered out as similar to these 100 REVIEWS/reviewID.txt files, count the word occurrences. 
#    Which 10 tweet words appear most frequently overall? 

	#    1209 time
	#    1125 new
	#     975 out
	#     934 good
	#     918 go
	#     906 night
	#     853 more
	#     829 GM
	#     795 warner
	#     776 get

# 7) Repeat the same analysis for any 100 customer reviews with low helpfulness scores of 0. 

	#     189 time
	#     152 all
	#     141 new
	#     131 warner
	#     117 get
	#     108 good
	#     104 love
	#     104 like
	#     101 more
	#      96 today

# ---- CODE ---- #

# 1) Get twitter dataset and unzip

benbow@f6linuxA5:~$ wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
--2021-11-19 05:05:31--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64
Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]
--2021-11-19 05:05:32--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 81363704 (78M) [application/zip]
Saving to: ‘trainingandtestdata.zip’

trainingandtestdata 100%[===================>]  77.59M  4.96MB/s    in 16s

2021-11-19 05:05:47 (4.93 MB/s) - ‘trainingandtestdata.zip’ saved [81363704/81363704]

benbow@f6linuxA5:~$ unzip trainingandtestdata.zip
Archive:  trainingandtestdata.zip
  inflating: testdata.manual.2009.06.14.csv
  inflating: training.1600000.processed.noemoticon.csv

benbow@f6linuxA5:~$ ls
a1                                         trainingandtestdata.zip
a2                                         worksheet1
a2.txt                                     ws1
a3                                         ws2
a4                                         ws3
amazon_reviews_us_Books_v1_02.tsv          ws4
myproject                                  ws4-work
sample.txt                                 ws5
temp-ws5                                   ws6
test_script.tmp                            ws7
testdata.manual.2009.06.14.csv             ws8
training.1600000.processed.noemoticon.csv

# Get 100 reviews with highest helpfulness scores and save reviews as REVIEWS/reviewID.txt

benbow@f6linuxA5:~$ git clone https://github.com/jenbow/a4.git
Cloning into 'a4'...
warning: You appear to have cloned an empty repository.

benbow@f6linuxA5:~$ cd a4

benbow@f6linuxA5:~/a4$ script a4.txt
Script started, file is a4.txt

benbow@f6linuxA5:~$ cd a4

benbow@f6linuxA5:~/a4$ head -n 1 ../training.1600000.processed.noemoticon.csv
"0","1467810369","Mon Apr 06 22:19:45 PDT 2009","NO_QUERY","_TheSpecialOne_","@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D"

benbow@f6linuxA5:~/a4$ mkdir REVIEWS

benbow@f6linuxA5:~/a4$ mkdir REVIEWS_UNHELPFUL

benbow@f6linuxA5:~/a4$ ls
REVIEWS  REVIEWS_UNHELPFUL  a4.txt

benbow@f6linuxA5:~/a4$ head -n 1 ../amazon_reviews_us_Books_v1_02.tsv | tr "\t" "\n" | cat -n
     1	marketplace
     2	customer_id
     3	review_id
     4	product_id
     5	product_parent
     6	product_title
     7	product_category
     8	star_rating
     9	helpful_votes
    10	total_votes
    11	vine
    12	verified_purchase
    13	review_headline
    14	review_body
    15	review_date

benbow@f6linuxA5:~/a4$ cut -f 3,9,14 ../amazon_reviews_us_Books_v1_02.tsv | sort -rnk 2 | head -n 100 > helpful_100.tmp

benbow@f6linuxA5:~/a4$ cut -f 3,9,14 ../amazon_reviews_us_Books_v1_02.tsv | sort -nk 2 | head -n 100 > unhelpful_100.tmp

# cut review IDs and store in file

benbow@f6linuxA5:~/a4$ cut -f 1 least_helpful_100.tmp > unhelpful_ids.tmp

benbow@f6linuxA5:~/a4$ cut -f 1 most_helpful_100.tmp > helpful_ids.tmp

benbow@f6linuxA5:~/a4$ ls
REVIEWS  REVIEWS_UNHELPFUL  a4.txt  cust-loop.sh  helpful_100.tmp  helpful_ids.tmp  log  unhelpful_100.tmp  unhelpful_ids.tmp

# review previous loop for creating files
benbow@f6linuxA5:~/a4$ cat cust-loop.sh
for id in $(cat cust_n100.txt)
do

    grep -w "^${id}" amazon-reviews-cust_id-star_rating-and-helpful_votes.tmp | cut -f 2,3,4 > CUSTOMERS/${id}.txt

done

# create files
benbow@f6linuxA5:~/a4$ for id in $(cat helpful_ids.tmp); do grep -w "^${id}" helpful_100.tmp | cut -f 3 > REVIEWS/${id}.txt; done
benbow@f6linuxA5:~/a4$ for id in $(cat unhelpful_ids.tmp); do grep -w "^${id}" unhelpful_100.tmp | cut -f 3 > REVIEWS_UNHELPFUL/${id}.txt; done

benbow@f6linuxA5:~/a4$ ls REVIEWS
R109NJJ7M1WVSA.txt  R1IGJOWWDLP3RS.txt  R20L37R2QXU1MN.txt  R2OOSWKHOA995R.txt  R38RXR8USISV94.txt  R3NO7IHYBFRHHB.txt  RH2NZEPRWAKTS.txt
R10XI57RK1Y3HJ.txt  R1JE512OED0O0U.txt  R22D7C0DULO855.txt  R2PCH4Z4ZC01PB.txt  R38X04K6Q42EE3.txt  R3RPPCPDM4QLM.txt   RHNZOTH3E02EZ.txt
R1273QROSPVC9Q.txt  R1K2ME7XQZRTN.txt   R259GTDWTK33M5.txt  R2S2WP2FQVEARG.txt  R396JHO923TTF8.txt  R3SH1UJ4DYJWBK.txt  RL345VSFPB7R4.txt
R17FRD3D03DHK8.txt  R1KJEEZ5RSS101.txt  R260B9ZTTD5DC2.txt  R2TT2WR0QO39G9.txt  R3A9IZQNNM9W3K.txt  R3URUU4G9A1FO1.txt  RPUSBNNCSTI4B.txt
R1899SEYW741FA.txt  R1LE29RYR173BR.txt  R27KG7MNJ5C9LD.txt  R2VDKZ4X1F992Q.txt  R3ANQNVFLDWTFF.txt  R3V2FRSXWS7JHB.txt  RPYQ6N2R7OXFB.txt
R1AVFGL39ARSM6.txt  R1LYD0LORZYG8M.txt  R28A60L3JTIZUI.txt  R2WDV1AW1J0512.txt  R3C5OUA56KKLZK.txt  R3W12GHFZU1Y2T.txt  RQBMW2UV8JBFJ.txt
R1BJOS745GHLAV.txt  R1MCT7PY7X515W.txt  R290Y7TOTIM8BP.txt  R2ZL0J5IEOC2RC.txt  R3D51K9T88JGSC.txt  R403HR4VL71K8.txt   RSNLSOVC98XTT.txt
R1C69UY8ZJF91E.txt  R1MSPHFMC7OKBV.txt  R29EIHM4KEYJWI.txt  R2ZUS4TQMBSOAL.txt  R3FEVO2OZ1H2VN.txt  R4CG3G94LE82O.txt   RTLG8BHPCNJY1.txt
R1CVDNT5FM236S.txt  R1NKU51ZEMHA7K.txt  R2AWM1ECF2OW56.txt  R31JSQX1A6IESE.txt  R3FFG0F0RKPR8H.txt  R6JFFQ7QD9S9E.txt   RUGSCP3XBNBUV.txt
R1CXKOPOW0K2I9.txt  R1OJBOGT5D8Q1D.txt  R2CT5DGXF6GYA9.txt  R35A7ZTFXW7G2E.txt  R3FHCVUBHKRF4L.txt  R6JXBPT5RFHNK.txt   RVFJEZZ01QP58.txt
R1EOM8Q95MESS5.txt  R1RJWJLHN1MLD7.txt  R2HN40RKHWH2DJ.txt  R35E602YIU3XL4.txt  R3G6528PWEDVRA.txt  R6PNB8TTTA6SM.txt
R1F3B3Q02IZ114.txt  R1T5UD2858KHTO.txt  R2HQ3WX26XBRFG.txt  R36D4QUKS1NBPK.txt  R3HQKUPKY7SCLU.txt  RBJLNES9V3UBR.txt
R1FFHE354D563L.txt  R1UW73W347V5X1.txt  R2KCPHK3RKSG6N.txt  R36V6DYQXY1PX0.txt  R3ITPVMB21C8QV.txt  RDHI2GR5Y70W4.txt
R1I7ARHZHTVP5I.txt  R1WLJRS5X06ZTS.txt  R2KEEEVV8Y1NFI.txt  R370A5CNKW3PRQ.txt  R3JLSN6LGREPGM.txt  RDMP45RYIRYSB.txt
R1IF16DWWMC0CU.txt  R1Y557JBALMN8K.txt  R2MLNFHGCJUWVF.txt  R37ZXODUGD65UE.txt  R3L67TB57GUJL3.txt  REZVCBI3L2685.txt

benbow@f6linuxA5:~/a4$ ls REVIEWS_UNHELPFUL
R10001OP8YVPFD.txt  R1004JXSNLWGIO.txt  R100BF7QHBJL7K.txt  R100GXG4Z9LVVL.txt  R100KRDD388BFL.txt  R100OPFEDPWW18.txt  R100WASBAG4F4X.txt
R10005LH1H4ZJN.txt  R1004W0VHDMZ55.txt  R100BFGB87BCKT.txt  R100HKZA0PG7PW.txt  R100KWS8DKH3EH.txt  R100ORU9P6UURM.txt  R100WOIRCUNISM.txt
R10005QPZS2TCW.txt  R100661DPVX3LJ.txt  R100BQBV47HYEM.txt  R100HL82N54048.txt  R100L2VM5LJR.txt    R100P866M4OVAE.txt  R100WYUD17Q5LQ.txt
R10009BW8E567W.txt  R1006PRQ6OUWPJ.txt  R100C27WLEIJSH.txt  R100HUG3W7XURD.txt  R100L6515H5213.txt  R100QEYU0S9NL7.txt  R100X5XZR26QUY.txt
R1000FV51MIN4U.txt  R1006RY6HFFHU1.txt  R100D0YAW82GPR.txt  R100HYLFOO4XDG.txt  R100LDMTRMRE78.txt  R100QV4Y7QXYIG.txt  R100XJ4174JBV0.txt
R1000H9YF4085I.txt  R1006ZMNRX88AD.txt  R100D1R9WZH322.txt  R100I0XQEM7T34.txt  R100LMRH56YP59.txt  R100RB8OBBQ7AO.txt  R100XJUDDMC4JW.txt
R1000L4A9EZ0JH.txt  R10073VOXF07YE.txt  R100DM7ISDKX48.txt  R100I3DJD9E1PP.txt  R100LYOUA6KVWO.txt  R100RO78KTM4PY.txt  R100XPJLWNK4DA.txt
R1000TG6R9RON9.txt  R1007FQUBRA11D.txt  R100DRTTBA7LN9.txt  R100ICH0FS3TD0.txt  R100MNX71OL8A2.txt  R100S6DXZ4H52P.txt  R100Y0EK9XOQ42.txt
R1001VQP3QZ3PZ.txt  R10096J9FHSWKO.txt  R100E2RSXRRSIN.txt  R100ILTU892QBM.txt  R100NDE7J81CSU.txt  R100SAQR650HDQ.txt  R100Y535C9PHJ9.txt
R1001X5MC3K02P.txt  R1009HF64YJI41.txt  R100E302YGLLIF.txt  R100IRVRD7ULSH.txt  R100NG6VZ7KEMW.txt  R100TB3A1FB8WW.txt  R100Y6MUDBEYJ.txt
R1001ZP33N1GEZ.txt  R1009SA43EQXTG.txt  R100EDHS49CEEF.txt  R100IUCYPVD1UR.txt  R100NHOPZSE1P5.txt  R100TH8048SU1E.txt
R1002CC0K7P0PP.txt  R100A0MRGL2V5G.txt  R100EQCY5DF5RL.txt  R100IWGP568RTU.txt  R100NTSZ95M3BA.txt  R100UHVBW4U5DL.txt
R1002ELHFL1W3B.txt  R100AFTOEPKP97.txt  R100F3YV0HVD8J.txt  R100JOSYU0NLN5.txt  R100OERLKB3Y0N.txt  R100UU3B8VULR5.txt
R1002FAEXYJTB.txt   R100AVKGGUXS92.txt  R100FHDB2E4YQ3.txt  R100JQZ2SB5TRA.txt  R100OLLMVY2QYF.txt  R100VTSPXFACJ7.txt
R10040VE8KTVXW.txt  R100B3232NVBD0.txt  R100FK3LUV8URO.txt  R100KH8T75WPLU.txt  R100OMHXHBTBJ9.txt  R100W3RMIKFN23.txt

# 2) lemmatization: remove endings "ing", "ed", "s"
benbow@f6linuxA5:~/a4$ mkdir REVIEWS_modified
benbow@f6linuxA5:~/a4$ for file in $(ls REVIEWS/); do id=$(echo ${file} | sed 's/.txt//'); sed -E 's/([[:alpha:]])ing([ .,;:])/\1\2/g' REVIEWS/${file} | sed -E 's/([[:alpha:]])s([ .,;:])/\1\2/g' | sed -E 's/([[:alpha:]])ed([ .,;:])/\1\2/g' > REVIEWS_modified/${id}-mod.txt; done
benbow@f6linuxA5:~/a4$ ls
REVIEWS  REVIEWS_UNHELPFUL  REVIEWS_modified  a4.txt  log
benbow@f6linuxA5:~/a4$ head -n 1 REVIEWS_modified/RVFJEZZ01QP58-mod.txt
Funny, most of the review on thi book are either ecstatic or disgust.  I see both side.  True, thi woman DOES think she ha reach it and know absolutely everyth; she trie to put the artistic proces (for any artist, amateur or professional, in any medium) in a box; she trie to deny that be an artist ha to involve any real work; she teache you to be selfish; and she i awfully repetitive.  THAT SAID, a few of her individual idea are so epiphanic (i that a word?) that if you come to any one of them for the first time, you will have got more than your money's worth.  Though the whole th IS a bit wishy-washy and new-agey, and though some thing she think are awfully vital just don't seem that huge to me, some of her point nevertheles can't be miss.  It goe on an individual basis--some people really ne some of thi stuff, other have seen it before.  Some of the exercise will show you someth, some will seem silly.  Give it a try, tailor it to your own ne.  The people who will get the most out of thi book are the many \\"silent poets\\" who have alway want to try their hand at draw or danc or sing but who haven't because they've been afraid of fail or of look silly.  Those with a happy, fulfill artistic life will roll their eye over it-but they should realize it i written specifically for people who are deal with a major block.  It's only fair to consider it with that particular audience in mind.

# 3) remove stop words, words with one or two characters, and punctuation symbols (commas, dots, colons, etc) 
benbow@f6linuxA5:~/a4$ nano words-to-remove.txt
benbow@f6linuxA5:~/a4$ cat remove-words-and-puncs.sh
	#!/usr/bin/env bash
	set -e

	# copying input file to output file to use in-place (-i) parameter of sed
	cp $1 $3

	# looping through input words
	for word in $(cat $2)
	do

	    # removing case-insensitive matches
	    sed -i -e "s/\<${word}\>//gI" $3
	
	done

	# removing punctuation, extra spaces, and space up front if any lines start with a space now
	tr -d '.,?;:()\-"/\\' < $3 | tr -s " " | sed 's/^ //' > word-removal.tmp && mv word-removal.tmp $3

benbow@f6linuxA5:~/a4$ ls REVIEWS_modified_2/
R109NJJ7M1WVSA-mod-2.txt  R1LYD0LORZYG8M-mod-2.txt  R2HN40RKHWH2DJ-mod-2.txt  R38RXR8USISV94-mod-2.txt  R3W12GHFZU1Y2T-mod-2.txt
R10XI57RK1Y3HJ-mod-2.txt  R1MCT7PY7X515W-mod-2.txt  R2HQ3WX26XBRFG-mod-2.txt  R38X04K6Q42EE3-mod-2.txt  R403HR4VL71K8-mod-2.txt
R1273QROSPVC9Q-mod-2.txt  R1MSPHFMC7OKBV-mod-2.txt  R2KCPHK3RKSG6N-mod-2.txt  R396JHO923TTF8-mod-2.txt  R4CG3G94LE82O-mod-2.txt
R17FRD3D03DHK8-mod-2.txt  R1NKU51ZEMHA7K-mod-2.txt  R2KEEEVV8Y1NFI-mod-2.txt  R3A9IZQNNM9W3K-mod-2.txt  R6JFFQ7QD9S9E-mod-2.txt
R1899SEYW741FA-mod-2.txt  R1OJBOGT5D8Q1D-mod-2.txt  R2MLNFHGCJUWVF-mod-2.txt  R3ANQNVFLDWTFF-mod-2.txt  R6JXBPT5RFHNK-mod-2.txt
R1AVFGL39ARSM6-mod-2.txt  R1RJWJLHN1MLD7-mod-2.txt  R2OOSWKHOA995R-mod-2.txt  R3C5OUA56KKLZK-mod-2.txt  R6PNB8TTTA6SM-mod-2.txt
R1BJOS745GHLAV-mod-2.txt  R1T5UD2858KHTO-mod-2.txt  R2PCH4Z4ZC01PB-mod-2.txt  R3D51K9T88JGSC-mod-2.txt  RBJLNES9V3UBR-mod-2.txt
R1C69UY8ZJF91E-mod-2.txt  R1UW73W347V5X1-mod-2.txt  R2S2WP2FQVEARG-mod-2.txt  R3FEVO2OZ1H2VN-mod-2.txt  RDHI2GR5Y70W4-mod-2.txt
R1CVDNT5FM236S-mod-2.txt  R1WLJRS5X06ZTS-mod-2.txt  R2TT2WR0QO39G9-mod-2.txt  R3FFG0F0RKPR8H-mod-2.txt  RDMP45RYIRYSB-mod-2.txt
R1CXKOPOW0K2I9-mod-2.txt  R1Y557JBALMN8K-mod-2.txt  R2VDKZ4X1F992Q-mod-2.txt  R3FHCVUBHKRF4L-mod-2.txt  REZVCBI3L2685-mod-2.txt
R1EOM8Q95MESS5-mod-2.txt  R20L37R2QXU1MN-mod-2.txt  R2WDV1AW1J0512-mod-2.txt  R3G6528PWEDVRA-mod-2.txt  RH2NZEPRWAKTS-mod-2.txt
R1F3B3Q02IZ114-mod-2.txt  R22D7C0DULO855-mod-2.txt  R2ZL0J5IEOC2RC-mod-2.txt  R3HQKUPKY7SCLU-mod-2.txt  RHNZOTH3E02EZ-mod-2.txt
R1FFHE354D563L-mod-2.txt  R259GTDWTK33M5-mod-2.txt  R2ZUS4TQMBSOAL-mod-2.txt  R3ITPVMB21C8QV-mod-2.txt  RL345VSFPB7R4-mod-2.txt
R1I7ARHZHTVP5I-mod-2.txt  R260B9ZTTD5DC2-mod-2.txt  R31JSQX1A6IESE-mod-2.txt  R3JLSN6LGREPGM-mod-2.txt  RPUSBNNCSTI4B-mod-2.txt
R1IF16DWWMC0CU-mod-2.txt  R27KG7MNJ5C9LD-mod-2.txt  R35A7ZTFXW7G2E-mod-2.txt  R3L67TB57GUJL3-mod-2.txt  RPYQ6N2R7OXFB-mod-2.txt
R1IGJOWWDLP3RS-mod-2.txt  R28A60L3JTIZUI-mod-2.txt  R35E602YIU3XL4-mod-2.txt  R3NO7IHYBFRHHB-mod-2.txt  RQBMW2UV8JBFJ-mod-2.txt
R1JE512OED0O0U-mod-2.txt  R290Y7TOTIM8BP-mod-2.txt  R36D4QUKS1NBPK-mod-2.txt  R3RPPCPDM4QLM-mod-2.txt   RSNLSOVC98XTT-mod-2.txt
R1K2ME7XQZRTN-mod-2.txt   R29EIHM4KEYJWI-mod-2.txt  R36V6DYQXY1PX0-mod-2.txt  R3SH1UJ4DYJWBK-mod-2.txt  RTLG8BHPCNJY1-mod-2.txt
R1KJEEZ5RSS101-mod-2.txt  R2AWM1ECF2OW56-mod-2.txt  R370A5CNKW3PRQ-mod-2.txt  R3URUU4G9A1FO1-mod-2.txt  RUGSCP3XBNBUV-mod-2.txt
R1LE29RYR173BR-mod-2.txt  R2CT5DGXF6GYA9-mod-2.txt  R37ZXODUGD65UE-mod-2.txt  R3V2FRSXWS7JHB-mod-2.txt  RVFJEZZ01QP58-mod-2.txt

benbow@f6linuxA5:~/a4$ head REVIEWS_modified_2/RVFJEZZ01QP58-mod-2.txt
Funny most review thi book ecstatic or disgust see both side True thi woman think she ha reach know absolutely everyth she trie put artistic proces artist amateur or professional medium box she trie deny artist ha involve real work she teache selfish she awfully repetitive SAID few her individual idea epiphanic word come one them first time got more than money's worth Though whole th bit wishywashy newagey though some thing she think awfully vital don't seem huge some her point nevertheles 't miss goe individual basissome people really ne some thi stuff other seen before Some exercise show someth some seem silly Give try tailor own ne people get most out thi book many silent poets alway want try their hand draw or danc or sing haven't because they've been afraid fail or look silly Those happy fulfill artistic life roll their eye over they should realize written specifically people deal major block 's only fair consider particular audience mind

# 4) Write a shell script that takes 2 filenames as parameters: REVIEWS/reviewID.txt and the tweets file you downloaded above
#    (training*.csv), compares each tweet text (6th column) against the review_body, and appends to the end of the 
#    REVIEWS/reviewID.txt file all tweets that have at least 2 words in common with the review_body.

# Overwrote with parallel version (oops). See final version below: 'append-if-2-common-words.sh'

# 5) Use the parallel command we have seen to perform this operation against all 100 REVIEWS/reviewID.txt files in parallel. 
#    Time your parallel execution with the time command. How long did it take?

benbow@f6linuxA5:~/a4$ ls REVIEWS/ | cut -f 1 -d "." > unique-review-IDs.txt
benbow@f6linuxA5:~/a4$ mkdir REVIEWS_appended
benbow@f6linuxA5:~/a4$ cp REVIEWS_modified_2/* REVIEWS_appended/
benbow@f6linuxA5:~/a4$ cut -f 6 -d "," ../*train*csv | tr -d '"' > just-tweet-bodies.txt

benbow@f6linuxA5:~/a4$ cat append-if-2-common-words.sh
	
	#!/usr/bin/env bash
	set -e
	
	# usage example: bash append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt
	
	# in-file path
	in_file="REVIEWS_appended/${1}-mod-2.txt"
	
	# getting sorted list of words in review (using review ID because needs unique name if run in parallel)
	tr " " "\n" < ${in_file} | sort -u > ${1}-review-words.tmp
	
	# setting default delimiter to newlines
	IFS=$'\n'
	
	# looping through tweets and counting how many words are shared with the review
	for tweet in $(cat just-tweet-bodies.txt)
	do
	
	    # getting sorted list of words in tweet (using review ID because needs unique name if run in parallel)
	    echo ${tweet} | tr " " "\n" | sort -u > ${1}-curr-tweet-words.tmp
	
	    # getting number shared between tweet and review
	    curr_shared=$(comm -12 ${1}-review-words.tmp ${1}-curr-tweet-words.tmp | wc -l)
	
	    # if greater than 1 shared word, appending tweet to current review
	    if [ ${curr_shared} -gt 1 ]; then
	        echo ${tweet} >> ${in_file}
	    fi
	
	    # removing tmp tweet file
	    rm ${1}-curr-tweet-words.tmp
	
	done
	
	# removing temp review words file
	rm ${1}-review-words.tmp
	
	# renaming review file
	mv ${in_file} REVIEWS_appended/${1}-appended.txt

benbow@f6linuxA5:~/a4$ head -n 100 just-tweet-bodies.copy > just-tweet-bodies.txt
benbow@f6linuxA5:~/a4$ time bash append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt

real	0m0.430s
user	0m0.445s
sys	0m0.213s

benbow@f6linuxA5:~/a4$ head -n 1000 just-tweet-bodies.copy > just-tweet-bodies.txt
benbow@f6linuxA5:~/a4$ time bash append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt

real	0m6.345s
user	0m4.764s
sys	0m4.082s

benbow@f6linuxA5:~/a4$ head -n 10000 just-tweet-bodies.copy > just-tweet-bodies.txt
benbow@f6linuxA5:~/a4$ time bash append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt

real	1m18.411s
user	0m48.230s
sys	1m1.651s

# Increased by a factor of 14.76 from n=100 to n=1000, then increased by a factor of 12.36 from n=1000 to n=10000 

# I struggled to install parallel, so I hopped over to a server where I could install it and ran: 
 
head -n 500 just-tweet-bodies.copy > just-tweet-bodies.txt
time parallel bash append-if-2-common-words.sh {1} just-tweet-bodies.txt :::: unique-review-IDs.txt

# It took 40 seconds with this small subset of 500 tweets. I think it would scale linearly with all 1,600,000 in the full data set, resulting in a runtime of about 35.6 hours.  

# 6) Amongst all the tweets you filtered out as similar to these 100 REVIEWS/reviewID.txt files, count the word occurrences. 
#    Which 10 tweet words appear most frequently overall? Note: if your most frequent words are insignificant stop words (like "if" "or" "and", try removing those first and repeating).

# Each appended-review file's first line is the review, all others are tweets so we can get the tweets with tail
tail -n +2 REVIEWS_appended/*-appended.txt > all-tweets-with-common-words.txt

# Using script to remove stop/small words and punctuation
bash remove-words-and-puncs.sh all-tweets-with-common-words.txt words-to-remove.txt all-tweets-with-common-words-modified.txt

# Then changing spaces to new lines, sorting, counting (adding sed at the end to comment out the lines for easier pasting)
tr " " "\n" < all-tweets-with-common-words-modified.txt | sort | uniq -c | sort -nrk 1 | head | sed 's/^/# /'
#    8807 the
#    1549 The
#    1357 so
#    1337 that
#    1209 time
#    1183
#    1125 new
#     975 out
#     957 was
#     934 good

# Then adding "the", "so", "that", and "was" to the words-to-remove.txt file

# Re-running word-removal
bash remove-words-and-puncs.sh all-tweets-with-common-words.txt words-to-remove.txt all-tweets-with-common-words-modified.txt

# Re-counting and removing empty lines
tr " " "\n" < all-tweets-with-common-words-modified.txt | sed '/^$/d' | sort | uniq -c | sort -nrk 1 | head | sed 's/^/# /'
#    1209 time
#    1125 new
#     975 out
#     934 good
#     918 go
#     906 night
#     853 more
#     829 GM
#     795 warner
#     776 get

# 7) Repeat the same analysis for any 100 customer reviews with low helpfulness scores of 0. Save them under directory 
#    REVIEWS_UNHELPFUL in files named as REVIEWS_UNHELPFUL/reviewID.txt

benbow@f6linuxA5:~/a4$ mkdir REVIEWS_UNHELPFUL_modified
benbow@f6linuxA5:~/a4$ for file in $(ls REVIEWS_UNHELPFUL/); do id=$(echo ${file} | sed 's/.txt//'); sed -E 's/([[:alpha:]])ing([ .,;:])/\1\2/g' REVIEWS_UNHELPFUL/${file} | sed -E 's/([[:alpha:]])s([ .,;:])/\1\2/g' | sed -E 's/([[:alpha:]])ed([ .,;:])/\1\2/g' > REVIEWS_UNHELPFUL_modified/${id}-mod.txt; done
benbow@f6linuxA5:~/a4$ mkdir REVIEWS_UNHELPFUL_modified_2
benbow@f6linuxA5:~/a4$ for file in $(ls REVIEWS_UNHELPFUL/); do id=$(echo ${file} | sed 's/.txt//'); bash remove-words-and-puncs.sh REVIEWS_UNHELPFUL_modified/${id}-mod.txt words-to-remove.txt REVIEWS_UNHELPFUL_modified_2/${id}-mod-2.txt; done
benbow@f6linuxA5:~/a4$ nano UNHELPFUL_append-if-2-common-words.sh

benbow@f6linuxA5:~/a4$ cat UNHELPFUL_append-if-2-common-words.sh

	#!/usr/bin/env bash
	set -e

	# usage example: bash UNHELPFUL_append-if-2-common-words.sh R109NJJ7M1WVSA just-tweet-bodies.txt

	# in-file path
	in_file="REVIEWS_UNHELPFUL_appended/${1}-mod-2.txt"
	
	# getting sorted list of words in review (using review ID because needs unique name if run in parallel)
	tr " " "\n" < ${in_file} | sort -u > ${1}-review-words.tmp
	
	# setting default delimiter to newlines
	IFS=$'\n'

	# looping through tweets and counting how many words are shared with the review
	for tweet in $(cat just-tweet-bodies.txt)
	do

	    # getting sorted list of words in tweet (using review ID because needs unique name if run in parallel)
	    echo ${tweet} | tr " " "\n" | sort -u > ${1}-curr-tweet-words.tmp

	    # getting number shared between tweet and review
	    curr_shared=$(comm -12 ${1}-review-words.tmp ${1}-curr-tweet-words.tmp | wc -l)

	    # if greater than 1 shared word, appending tweet to current review
	    if [ ${curr_shared} -gt 1 ]; then
	        echo ${tweet} >> ${in_file}
	    fi

	    # removing tmp tweet file
	    rm ${1}-curr-tweet-words.tmp

	done

	# removing temp review words file
	rm ${1}-review-words.tmp

	# renaming review file
	mv ${in_file} REVIEWS_UNHELPFUL_appended/${1}-appended.txt

benbow@f6linuxA5:~/a4$ ls REVIEWS_UNHELPFUL/ | cut -f 1 -d "." > UNHELPFUL_unique-review-IDs.txt
benbow@f6linuxA5:~/a4$ mkdir REVIEWS_UNHELPFUL_appended
benbow@f6linuxA5:~/a4$ cp REVIEWS_UNHELPFUL_modified_2/* REVIEWS_UNHELPFUL_appended/

# Repeating previous steps with parallel on UNHELPFUL data
time parallel bash UNHELPFUL_append-if-2-common-words.sh {1} just-tweet-bodies.txt :::: UNHELPFUL_unique-review-IDs.txt

tail -n +2 REVIEWS_UNHELPFUL_appended/*-appended.txt > UNHELPFUL_all-tweets-with-common-words.txt

bash remove-words-and-puncs.sh UNHELPFUL_all-tweets-with-common-words.txt words-to-remove.txt UNHELPFUL_all-tweets-with-common-words-modified.txt

tr " " "\n" < UNHELPFUL_all-tweets-with-common-words-modified.txt | sed '/^$/d' | sort | uniq -c | sort -nrk 1 | head | sed 's/^/# /'

#     189 time
#     152 all
#     141 new
#     131 warner
#     117 get
#     108 good
#     104 love
#     104 like
#     101 more
#      96 today
